"""Panel evaluation pipeline with provider adapters and aggregation."""

from __future__ import annotations

import asyncio
import json
import os
import re
from collections import Counter
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional, Sequence
from uuid import UUID

from dotenv import load_dotenv

try:  # Optional at runtime â€“ fall back to stubs when unavailable
    import openai
    from typing import TYPE_CHECKING
    if TYPE_CHECKING:
        AsyncOpenAI = openai.AsyncOpenAI
    else:
        AsyncOpenAI = None
except Exception:  # pragma: no cover - dependency optional
    openai = None  # type: ignore
    AsyncOpenAI = None  # type: ignore

load_dotenv()

from ..models import (
    Claim,
    Evidence,
    ModelAssessment,
    PanelModelVerdict,
    PanelResult,
    PanelSummary,
    PanelVerdict,
    TimeWindow,
    VerdictType,
)
from .agentic_research import AgenticResearcher, SharedEvidencePool

DEFAULT_PANEL_MODELS: List[str] = [
    "gpt-4o",  # OpenAI's latest model (was gpt-5 which doesn't exist)
    "grok-3",  # xAI's latest (grok-beta deprecated Sept 2025)
    "gemini-2.0-flash-exp",  # Google's latest Gemini
    "claude-sonnet-4-20250514",  # Anthropic's latest Claude Sonnet 4
]

SYSTEM_PROMPT = """You are an objective fact-checking assistant. For each claim, you must provide BOTH an approval argument and a refusal argument with evidence and confidence.

Respond **only** with a JSON object matching:
{
  "provider_id": "provider:model",
  "approval_argument": {
    "argument": "50-500 words explaining why the claim might be true, with evidence IDs",
    "evidence_ids": ["evidence_id", ...],
    "confidence": 0.0-1.0
  },
  "refusal_argument": {
    "argument": "50-500 words explaining why the claim might be false, with evidence IDs",
    "evidence_ids": ["evidence_id", ...],
    "confidence": 0.0-1.0
  }
}

Provide balanced analysis with evidence citations for both perspectives. Higher confidence means stronger evidence support."""


def build_normalized_prompt(claim: Claim, window: Optional[TimeWindow]) -> Dict[str, Any]:
    """Construct normalized prompt payload for provider adapters."""
    window = window or TimeWindow()
    evidence_payload: List[Dict[str, Any]] = []
    for evidence in sorted(claim.evidence, key=lambda e: e.published_at or datetime.min):
        evidence_payload.append(
            {
                "id": str(evidence.id),
                "publisher": evidence.publisher,
                "snippet": evidence.snippet,
                "url": evidence.url,
                "published_at": evidence.published_at.isoformat()
                if evidence.published_at
                else None,
            }
        )

    return {
        "schema": "truce.panel.v1",
        "claim": {
            "id": str(claim.id),
            "text": claim.text,
            "topic": claim.topic,
            "entities": claim.entities,
        },
        "time_window": {
            "start": window.start.isoformat() if window.start else None,
            "end": window.end.isoformat() if window.end else None,
        },
        "evidence": evidence_payload,
        "evidence_count": len(evidence_payload),
        "generated_at": datetime.utcnow().isoformat(timespec="seconds"),
    }


class BaseProviderAdapter:
    """Base class for provider adapters with structured fallbacks."""

    provider_id: str

    def __init__(self, model: str) -> None:
        self.model = model

    async def evaluate(
        self,
        claim: Claim,
        prompt: Dict[str, Any],
        evidence_lookup: Dict[str, UUID],
    ) -> PanelModelVerdict:
        from ..models import ArgumentWithEvidence
        
        payload = await self._call_provider(prompt)
        parsed = _ensure_payload_dict(payload)
        
        # Parse approval argument
        approval_data = parsed.get("approval_argument", {})
        approval_arg = approval_data.get("argument", "") or _fallback_argument(
            self.provider_id, self.model, prompt, "approval"
        )
        # Ensure argument meets length requirements (50-1000 chars)
        if len(approval_arg) > 1000:
            approval_arg = approval_arg[:1000]
        elif len(approval_arg) < 50:
            approval_arg = _pad_argument(approval_arg, "approval")
        approval_evidence = _map_citations(approval_data.get("evidence_ids", []), evidence_lookup)
        approval_confidence = _parse_confidence(approval_data.get("confidence"), default=0.5)
        
        # Parse refusal argument
        refusal_data = parsed.get("refusal_argument", {})
        refusal_arg = refusal_data.get("argument", "") or _fallback_argument(
            self.provider_id, self.model, prompt, "refusal"
        )
        # Ensure argument meets length requirements (50-1000 chars)
        if len(refusal_arg) > 1000:
            refusal_arg = refusal_arg[:1000]
        elif len(refusal_arg) < 50:
            refusal_arg = _pad_argument(refusal_arg, "refusal")
        refusal_evidence = _map_citations(refusal_data.get("evidence_ids", []), evidence_lookup)
        refusal_confidence = _parse_confidence(refusal_data.get("confidence"), default=0.5)

        return PanelModelVerdict(
            provider_id=self.provider_id,
            model=self.model,
            approval_argument=ArgumentWithEvidence(
                argument=approval_arg,
                evidence_ids=approval_evidence,
                confidence=approval_confidence,
            ),
            refusal_argument=ArgumentWithEvidence(
                argument=refusal_arg,
                evidence_ids=refusal_evidence,
                confidence=refusal_confidence,
            ),
            raw=parsed,
        )

    async def _call_provider(self, prompt: Dict[str, Any]) -> Any:
        try:
            response = await self._invoke(prompt)
            if response is None:
                raise ValueError("Empty provider response")
            return response
        except Exception as exc:
            return self._fallback(prompt, error=str(exc))

    async def _invoke(self, prompt: Dict[str, Any]) -> Any:
        raise NotImplementedError

    def _fallback(self, prompt: Dict[str, Any], error: str = "") -> Dict[str, Any]:
        return _generate_stub_payload(self.provider_id, self.model, prompt, error=error)


class GPTProviderAdapter(BaseProviderAdapter):
    """Adapter for OpenAI GPT models."""

    def __init__(self, model: str) -> None:
        super().__init__(model)
        self.provider_id = f"openai:{model}"
        self._client: Optional[Any] = None if openai else None

    async def _invoke(self, prompt: Dict[str, Any]) -> Any:  # pragma: no cover - network call
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or openai is None:
            raise RuntimeError("OpenAI API key not configured")

        if self._client is None:
            self._client = openai.AsyncOpenAI(api_key=api_key)

        serialized = json.dumps(prompt, ensure_ascii=False)

        # Use standard chat completions for all OpenAI models
        completion = await self._client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": serialized},
            ],
            max_tokens=900,
            temperature=0.1,
        )
        content = completion.choices[0].message.content if completion.choices else ""
        return _ensure_payload_dict(content)


class GrokProviderAdapter(BaseProviderAdapter):
    """Adapter for xAI Grok models."""

    def __init__(self, model: str) -> None:
        super().__init__(model)
        self.provider_id = f"xai:{model}"
        self._client: Optional[Any] = None if openai else None

    async def _invoke(self, prompt: Dict[str, Any]) -> Any:  # pragma: no cover - network call
        api_key = os.getenv("XAI_API_KEY")
        if not api_key or openai is None:
            raise RuntimeError("XAI API key not configured or OpenAI library not available")

        if self._client is None:
            # XAI uses OpenAI-compatible API
            self._client = openai.AsyncOpenAI(
                api_key=api_key,
                base_url="https://api.x.ai/v1"
            )

        serialized = json.dumps(prompt, ensure_ascii=False)

        completion = await self._client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": serialized},
            ],
            max_tokens=900,
            temperature=0.1,
        )
        content = completion.choices[0].message.content if completion.choices else ""
        return _ensure_payload_dict(content)


class GeminiProviderAdapter(BaseProviderAdapter):
    """Adapter for Google Gemini models via OpenAI-compatible endpoint."""

    def __init__(self, model: str) -> None:
        super().__init__(model)
        self.provider_id = f"google:{model}"
        self._client: Optional[Any] = None if openai else None

    async def _invoke(self, prompt: Dict[str, Any]) -> Any:  # pragma: no cover - network call
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key or openai is None:
            raise RuntimeError("Google API key not configured or OpenAI library not available")

        if self._client is None:
            # Using OpenAI-compatible interface for Gemini via a proxy or direct integration
            # Note: This may need adjustment based on actual Google API implementation
            self._client = openai.AsyncOpenAI(
                api_key=api_key,
                base_url="https://generativelanguage.googleapis.com/v1beta/openai/"  # Hypothetical endpoint
            )

        serialized = json.dumps(prompt, ensure_ascii=False)

        completion = await self._client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": serialized},
            ],
            max_tokens=900,
            temperature=0.1,
        )
        content = completion.choices[0].message.content if completion.choices else ""
        return _ensure_payload_dict(content)


class AnthropicProviderAdapter(BaseProviderAdapter):
    """Adapter for Anthropic Claude models."""

    def __init__(self, model: str) -> None:
        super().__init__(model)
        self.provider_id = f"anthropic:{model}"
        self._client: Optional[Any] = None

    async def _invoke(self, prompt: Dict[str, Any]) -> Any:  # pragma: no cover - network call
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise RuntimeError("Anthropic API key not configured")

        try:
            import anthropic
            if self._client is None:
                self._client = anthropic.AsyncAnthropic(api_key=api_key)
        except ImportError:
            raise RuntimeError("Anthropic library not available (pip install anthropic)")

        serialized = json.dumps(prompt, ensure_ascii=False)
        
        # Use the model name as-is (should be full Anthropic model name)
        anthropic_model = self.model

        message = await self._client.messages.create(
            model=anthropic_model,
            max_tokens=900,
            temperature=0.1,
            system=SYSTEM_PROMPT,
            messages=[
                {"role": "user", "content": serialized}
            ]
        )
        content = message.content[0].text if message.content else ""
        return _ensure_payload_dict(content)


def _resolve_adapter(model_name: str) -> BaseProviderAdapter:
    lowered = model_name.lower()
    if lowered.startswith("gpt"):
        return GPTProviderAdapter(model_name)
    if lowered.startswith("grok"):
        return GrokProviderAdapter(model_name)
    if lowered.startswith("gemini"):
        return GeminiProviderAdapter(model_name)
    if "sonnet" in lowered or "claude" in lowered:
        return AnthropicProviderAdapter(model_name)
    # Unknown providers fall back to OpenAI-style adapter
    return GPTProviderAdapter(model_name)


async def run_agentic_panel_evaluation(
    claim: Claim,
    models: Sequence[str],
    time_window: Optional[TimeWindow] = None,
    session_id: Optional[str] = None,
    mcp_server_url: Optional[str] = None,
) -> PanelResult:
    """
    Run agentic panel evaluation where each agent conducts independent research.
    
    Args:
        claim: The claim to evaluate (may have minimal or no initial evidence)
        models: List of model names to use as panel agents
        time_window: Optional time window for research
        session_id: Optional session ID for progress updates
        mcp_server_url: URL of the FastMCP Brave Search server
    
    Returns:
        PanelResult with independently researched evidence and verdicts
    """
    selected_models = list(models) if models else DEFAULT_PANEL_MODELS
    evidence_pool = SharedEvidencePool()
    
    # Phase 1: Independent Agentic Research
    if session_id:
        from ..main import emit_agent_update
        await emit_agent_update(
            session_id, 
            "Agentic Panel Coordinator", 
            "Starting independent research phase", 
            f"Each of {len(selected_models)} agents will conduct independent research on: {claim.text}",
            "research_phase",
            []
        )
    
    # Create researchers for each model/agent
    researchers = []
    research_tasks = []
    
    for model_name in selected_models:
        researcher = AgenticResearcher(
            agent_name=f"{model_name}_researcher",
            mcp_server_url=mcp_server_url,
            max_search_turns=5,
            max_sources_per_turn=8
        )
        researchers.append((model_name, researcher))
        
        # Start research task
        task = researcher.conduct_research(claim, time_window, session_id)
        research_tasks.append(task)
    
    # Wait for all research to complete
    research_results = await asyncio.gather(*research_tasks, return_exceptions=True)
    
    # Collect all evidence in shared pool
    total_evidence_collected = 0
    for i, (model_name, researcher) in enumerate(researchers):
        evidence_list = research_results[i]
        if isinstance(evidence_list, Exception):
            print(f"Research failed for {model_name}: {evidence_list}")
            evidence_list = []
        
        added_count = await evidence_pool.add_evidence(evidence_list, model_name)
        total_evidence_collected += added_count
    
    if session_id:
        evidence_summary = evidence_pool.get_evidence_summary()
        await emit_agent_update(
            session_id,
            "Agentic Panel Coordinator",
            f"Research phase complete: {total_evidence_collected} evidence items collected",
            f"Collected evidence from {evidence_summary['unique_domains']} domains and {evidence_summary['unique_publishers']} publishers",
            "research_complete",
            list(evidence_summary['publishers'][:5])
        )
    
    # Phase 2: Create enriched claim with all collected evidence
    all_evidence = evidence_pool.get_all_evidence()
    enriched_claim = Claim(
        id=claim.id,
        text=claim.text,
        topic=claim.topic,
        entities=claim.entities,
        evidence=all_evidence,  # Use all researched evidence
    )
    
    # Phase 3: Independent Verdict Formation
    if session_id:
        await emit_agent_update(
            session_id,
            "Agentic Panel Coordinator",
            "Starting verdict formation phase",
            f"Each agent will independently analyze all {len(all_evidence)} evidence items to form their verdict",
            "verdict_phase",
            []
        )
    
    # Build prompt with all collected evidence
    prompt = build_normalized_prompt(enriched_claim, time_window)
    evidence_lookup = {str(ev.id): ev.id for ev in all_evidence}
    
    # Get verdicts from each model independently
    panel_models: List[PanelModelVerdict] = []
    for model_name in selected_models:
        if session_id:
            await emit_agent_update(
                session_id,
                f"{model_name} Agent",
                f"Analyzing evidence for independent verdict",
                f"Evaluating {len(all_evidence)} evidence sources to determine claim veracity",
                "verdict_analysis",
                []
            )
        
        adapter = _resolve_adapter(model_name)
        verdict = await adapter.evaluate(enriched_claim, prompt, evidence_lookup)
        panel_models.append(verdict)
    
    # Phase 4: Update original claim with collected evidence
    claim.evidence.extend(all_evidence)
    # Remove duplicates based on URL while preserving order
    seen_urls = set()
    unique_evidence = []
    for evidence in claim.evidence:
        if evidence.url not in seen_urls:
            seen_urls.add(evidence.url)
            unique_evidence.append(evidence)
    claim.evidence = unique_evidence
    
    # Phase 5: Aggregate Panel Results
    summary = aggregate_panel(panel_models)
    
    if session_id:
        verdict_str = summary.verdict.value if summary.verdict else "mixed"
        await emit_agent_update(
            session_id,
            "Agentic Panel Coordinator",
            f"Panel evaluation complete: {verdict_str}",
            f"Final verdict: {verdict_str} with {summary.support_confidence:.2f} support / {summary.refute_confidence:.2f} refute confidence from {summary.model_count} independent agents",
            "complete",
            []
        )
    
    return PanelResult(prompt=prompt, models=panel_models, summary=summary)


async def run_panel_evaluation(
    claim: Claim,
    models: Sequence[str],
    time_window: Optional[TimeWindow] = None,
    session_id: Optional[str] = None,
    enable_agentic_research: bool = True,
    mcp_server_url: Optional[str] = None,
) -> PanelResult:
    """
    Run panel evaluation with optional agentic research.
    
    Args:
        claim: The claim to evaluate
        models: List of model names to use as panel agents
        time_window: Optional time window for evaluation
        session_id: Optional session ID for progress updates
        enable_agentic_research: Whether to enable agentic research mode
        mcp_server_url: URL of the FastMCP Brave Search server
    
    Returns:
        PanelResult with verdicts and evidence
    """
    if enable_agentic_research:
        return await run_agentic_panel_evaluation(
            claim, models, time_window, session_id, mcp_server_url
        )
    else:
        # Original panel evaluation (requires existing evidence)
        if not claim.evidence:
            raise ValueError("Cannot run panel evaluation without evidence")

        selected_models = list(models) if models else DEFAULT_PANEL_MODELS
        prompt = build_normalized_prompt(claim, time_window)
        evidence_lookup = {str(ev.id): ev.id for ev in claim.evidence}

        panel_models: List[PanelModelVerdict] = []
        for model_name in selected_models:
            adapter = _resolve_adapter(model_name)
            verdict = await adapter.evaluate(claim, prompt, evidence_lookup)
            panel_models.append(verdict)

        summary = aggregate_panel(panel_models)
        return PanelResult(prompt=prompt, models=panel_models, summary=summary)


def aggregate_panel(models: Sequence[PanelModelVerdict]) -> PanelSummary:
    """Aggregate per-model verdicts by averaging approval/refusal confidences."""
    if not models:
        return PanelSummary(
            support_confidence=0.0,
            refute_confidence=0.0,
            model_count=0,
            verdict=PanelVerdict.UNKNOWN,
        )

    # Calculate average confidence scores across all models
    total_approval = sum(model.approval_argument.confidence for model in models)
    total_refusal = sum(model.refusal_argument.confidence for model in models)
    
    support_confidence = total_approval / len(models)
    refute_confidence = total_refusal / len(models)
    
    # Determine verdict based on which confidence is higher
    if support_confidence > refute_confidence:
        if support_confidence >= 0.7:
            verdict = PanelVerdict.TRUE
        elif support_confidence >= 0.5:
            verdict = PanelVerdict.MIXED
        else:
            verdict = PanelVerdict.UNKNOWN
    elif refute_confidence > support_confidence:
        if refute_confidence >= 0.7:
            verdict = PanelVerdict.FALSE
        elif refute_confidence >= 0.5:
            verdict = PanelVerdict.MIXED
        else:
            verdict = PanelVerdict.UNKNOWN
    else:
        verdict = PanelVerdict.MIXED

    return PanelSummary(
        support_confidence=round(support_confidence, 4),
        refute_confidence=round(refute_confidence, 4),
        model_count=len(models),
        verdict=verdict,
    )


def panel_result_to_assessments(panel: PanelResult) -> List[ModelAssessment]:
    """Convert panel verdicts into legacy ModelAssessment structures."""
    assessments: List[ModelAssessment] = []
    
    for model in panel.models:
        # Use the stronger argument (higher confidence) as the primary verdict
        if model.approval_argument.confidence > model.refusal_argument.confidence:
            verdict = VerdictType.SUPPORTS
            confidence = model.approval_argument.confidence
            rationale = f"Approval: {model.approval_argument.argument}"
            citations = model.approval_argument.evidence_ids
        elif model.refusal_argument.confidence > model.approval_argument.confidence:
            verdict = VerdictType.REFUTES
            confidence = model.refusal_argument.confidence
            rationale = f"Refusal: {model.refusal_argument.argument}"
            citations = model.refusal_argument.evidence_ids
        else:
            verdict = VerdictType.MIXED
            confidence = (model.approval_argument.confidence + model.refusal_argument.confidence) / 2
            rationale = f"Mixed: Approval ({model.approval_argument.confidence:.2f}) - {model.approval_argument.argument[:100]}... Refusal ({model.refusal_argument.confidence:.2f}) - {model.refusal_argument.argument[:100]}..."
            citations = list(set(model.approval_argument.evidence_ids + model.refusal_argument.evidence_ids))
        
        assessments.append(
            ModelAssessment(
                model_name=model.model,
                verdict=verdict,
                confidence=confidence,
                citations=citations,
                rationale=rationale,
            )
        )
    return assessments


async def create_mock_assessments(claim: Claim) -> List[ModelAssessment]:
    """Generate mock assessments via the panel pipeline for demos/tests."""
    panel = await run_panel_evaluation(claim, DEFAULT_PANEL_MODELS)
    return panel_result_to_assessments(panel)


def _ensure_payload_dict(raw: Any) -> Dict[str, Any]:
    if isinstance(raw, dict):
        return raw
    if isinstance(raw, str):
        text = raw.strip()
        if not text:
            raise ValueError("Empty payload")
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            block = _extract_json_block(text)
            if block:
                return json.loads(block)
    raise ValueError("Could not parse provider payload")


def _extract_json_block(text: str) -> Optional[str]:
    match = re.search(r"\{.*\}", text, re.DOTALL)
    return match.group(0) if match else None


def _parse_panel_verdict(value: Any) -> PanelVerdict:
    if not value:
        return PanelVerdict.UNKNOWN
    normalized = str(value).strip().lower()
    if normalized in {"true", "supports", "supporting"}:
        return PanelVerdict.TRUE
    if normalized in {"false", "refutes", "refuting"}:
        return PanelVerdict.FALSE
    if normalized == "mixed":
        return PanelVerdict.MIXED
    if normalized == "unknown" or normalized == "uncertain":
        return PanelVerdict.UNKNOWN
    return PanelVerdict.UNKNOWN


def _map_citations(
    values: Optional[Iterable[Any]], evidence_lookup: Dict[str, UUID]
) -> List[UUID]:
    if not values:
        return []
    mapped: List[UUID] = []
    for value in values:
        if isinstance(value, UUID):
            mapped.append(value)
            continue
        if not value:
            continue
        token = str(value)
        for evidence_id, uuid_value in evidence_lookup.items():
            if token == evidence_id or token in evidence_id:
                mapped.append(uuid_value)
                break
    # Remove duplicates while preserving order
    seen: set[UUID] = set()
    result: List[UUID] = []
    for item in mapped:
        if item in seen:
            continue
        seen.add(item)
        result.append(item)
    return result


def _parse_confidence(value: Any, default: float = 0.5) -> float:
    """Parse confidence value ensuring it's within 0-1 range."""
    if value is None:
        return default
    try:
        conf = float(value)
        return max(0.0, min(1.0, conf))
    except (TypeError, ValueError):
        return default


def _fallback_argument(
    provider_id: str,
    model: str,
    prompt: Dict[str, Any],
    argument_type: str,
) -> str:
    """Generate fallback argument when provider response is incomplete."""
    claim_text = prompt.get("claim", {}).get("text", "the claim")
    evidence_count = prompt.get("evidence_count", 0)
    
    if argument_type == "approval":
        return (
            f"{provider_id} ({model}) fallback approval argument. "
            f"Analyzing {evidence_count} evidence item(s) for potential support of: {claim_text}. "
            "This argument is generated locally due to incomplete provider response."
        )
    else:
        return (
            f"{provider_id} ({model}) fallback refusal argument. "
            f"Analyzing {evidence_count} evidence item(s) for potential refutation of: {claim_text}. "
            "This argument is generated locally due to incomplete provider response."
        )


def _pad_argument(text: str, argument_type: str) -> str:
    """Pad short arguments to meet minimum length requirements."""
    if len(text) >= 50:
        # If already long enough, just ensure it's not too long
        return text[:1000] if len(text) > 1000 else text
    
    filler = (
        f" This {argument_type} argument includes analysis of available evidence "
        "with explicit citation of source identifiers for transparency."
    )
    combined = (text or f"No {argument_type} argument provided.") + filler
    # Ensure we don't exceed max length
    return combined[:1000] if len(combined) > 1000 else (combined if len(combined) >= 50 else combined.ljust(50, " "))


def _generate_stub_payload(
    provider_id: str,
    model: str,
    prompt: Dict[str, Any],
    error: str = "",
) -> Dict[str, Any]:
    evidence = prompt.get("evidence", [])
    evidence_ids = [ev["id"] for ev in evidence[:3]] if evidence else []
    profile = _stub_profile(provider_id)
    
    # Build evidence sources text
    evidence_sources = []
    for ev in evidence[:3]:
        publisher = ev.get("publisher", "Unknown")
        evidence_sources.append(publisher)
    sources_text = ", ".join(evidence_sources) if evidence_sources else "no sources"
    
    approval_arg = (
        f"{provider_id} ({model}) stub approval analysis. "
        f"Reviewed {len(evidence)} evidence items from {sources_text}. "
        "This is a deterministic offline assessment for testing."
    )
    
    refusal_arg = (
        f"{provider_id} ({model}) stub refusal analysis. "
        f"Reviewed {len(evidence)} evidence items from {sources_text}. "
        "This is a deterministic offline assessment for testing."
    )
    
    if error:
        approval_arg += f" Note: {error}."
        refusal_arg += f" Note: {error}."

    payload: Dict[str, Any] = {
        "provider_id": provider_id,
        "approval_argument": {
            "argument": _pad_argument(approval_arg, "approval"),
            "evidence_ids": evidence_ids,
            "confidence": profile["approval_confidence"],
        },
        "refusal_argument": {
            "argument": _pad_argument(refusal_arg, "refusal"),
            "evidence_ids": evidence_ids,
            "confidence": profile["refusal_confidence"],
        },
    }
    return payload


def _stub_profile(provider_id: str) -> Dict[str, Any]:
    """Generate stub confidence profiles for testing."""
    if provider_id.startswith("openai"):
        return {"approval_confidence": 0.75, "refusal_confidence": 0.25}
    if provider_id.startswith("xai"):
        return {"approval_confidence": 0.30, "refusal_confidence": 0.70}
    if provider_id.startswith("google"):
        return {"approval_confidence": 0.55, "refusal_confidence": 0.45}
    if provider_id.startswith("anthropic"):
        return {"approval_confidence": 0.65, "refusal_confidence": 0.35}
    return {"approval_confidence": 0.60, "refusal_confidence": 0.40}


